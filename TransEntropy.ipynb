{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rute/.conda/envs/pesquisa/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime as dt\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import scipy.integrate as integrate\n",
    "import seaborn as sns\n",
    "from scipy.stats import gaussian_kde\n",
    "import scipy.signal as sig\n",
    "from IPython.display import clear_output\n",
    "from graphviz import Digraph\n",
    "from threading import Thread\n",
    "import threading\n",
    "import queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "almNum = 3\n",
    "ocorr = 100\n",
    "t_sample = 60\n",
    "base_hour = dt.datetime(2018, 1,9,9,0,0)\n",
    "occor_num = 0\n",
    "\n",
    "#Settings alarms\n",
    "a = np.zeros(1, dtype=int)\n",
    "ocorr = 10\n",
    "duration = 120 #120sec\n",
    "hour_init = base_hour\n",
    "\n",
    "b = np.zeros(1, dtype=int)\n",
    "pAb =0.2\n",
    "delay_b = 60 #seconds\n",
    "duration_b = 120\n",
    "\n",
    "c = np.zeros(1, dtype=int)\n",
    "pAc = 0.6\n",
    "delay_c = 15 #seconds\n",
    "duration_c = 120\n",
    "\n",
    "#Alarm Series Generation, A (cause), B(Effect). C(Effect)\n",
    "while(occor_num < ocorr): \n",
    "    \n",
    "    #A generation - begin\n",
    "    srtd_hour = random.normalvariate(3, 1)\n",
    "    srtd_hour_begin = hour_init + dt.timedelta(hours=srtd_hour)\n",
    "    srtd_hour_end = srtd_hour_begin + dt.timedelta(seconds = duration)\n",
    "\n",
    "    idx_init_a_occor = int(math.ceil((srtd_hour_begin - base_hour).total_seconds()/t_sample))\n",
    "    idx_end_a_occor = int(math.ceil((srtd_hour_end - base_hour).total_seconds()/t_sample))\n",
    "        \n",
    "    if(idx_end_a_occor > a.size):\n",
    "            a.resize(idx_end_a_occor)\n",
    "\n",
    "    for i in np.arange(idx_init_a_occor, idx_end_a_occor + 1):\n",
    "        a[i-1] = 1\n",
    "    #A generation - end\n",
    "         \n",
    "    \n",
    "    #B generation begin\n",
    "    srtd_prob_b = random.uniform(0,1)  \n",
    "    if srtd_prob_b <= pAb:\n",
    "        srtd_hour_begin_b = srtd_hour_begin + dt.timedelta(hours = delay_b/3600)\n",
    "        srtd_hour_end_b = srtd_hour_begin_b + dt.timedelta(seconds=duration_b)\n",
    "\n",
    "        idx_init_b_occor = int(math.ceil((srtd_hour_begin_b - base_hour).total_seconds()/t_sample))\n",
    "        idx_end_b_occor = int(math.ceil((srtd_hour_end_b - base_hour).total_seconds()/t_sample))\n",
    "\n",
    "        if(idx_end_b_occor > b.size):\n",
    "                b.resize(idx_end_b_occor)\n",
    "\n",
    "        for j in np.arange(idx_init_b_occor, idx_end_b_occor +1):\n",
    "            b[j-1] =  1  \n",
    "\n",
    "    #B generation end\n",
    "\n",
    "    #C generation begin\n",
    "    srtd_prob_c = random.uniform(0,1)\n",
    "    if srtd_prob_c <= pAc:\n",
    "        srtd_hour_begin_c = srtd_hour_begin + dt.timedelta(hours = delay_b/3600)\n",
    "        srtd_hour_end_c = srtd_hour_begin_c + dt.timedelta(seconds=duration_c)\n",
    "\n",
    "        idx_init_c_occor = int(math.ceil((srtd_hour_begin_c - base_hour).total_seconds()/t_sample))\n",
    "        idx_end_c_occor = int(math.ceil((srtd_hour_end_c - base_hour).total_seconds()/t_sample))\n",
    "        \n",
    "        if(idx_end_c_occor > c.size):\n",
    "            c.resize(idx_end_c_occor)\n",
    "\n",
    "        for j in np.arange(idx_init_c_occor, idx_end_c_occor +1):\n",
    "            c[j-1] = 1    \n",
    "    #C generation end\n",
    "    \n",
    "    hour_init = srtd_hour_begin\n",
    "    occor_num = occor_num + 1\n",
    "\n",
    "#Making series the same length\n",
    "max_len = max(a, b, c, key=len).size\n",
    "a = np.concatenate([a, np.zeros(max_len - a.size)])\n",
    "b = np.concatenate([b, np.zeros(max_len - b.size)])\n",
    "c = np.concatenate([c, np.zeros(max_len - c.size)])\n",
    "sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization in a seaborn corr mamtrix \n",
    "def plot_mat_corrs(figsize, annot, matrix):\n",
    "    f, ax = plt.subplots(figsize=figsize)\n",
    "    cmap = sns.diverging_palette(150, 275, s=80, l=55, as_cmap=True)\n",
    "    sns.heatmap(matrix, cmap=cmap, center=0, annot=annot, vmin = 0, vmax=max(matrix.max()))\n",
    "    plt.yticks(rotation=45)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization by graph\n",
    "def graph(df, limite, eng = 'dot'):\n",
    "    g = Digraph(engine=eng)\n",
    "    for k, row in enumerate(df.index):\n",
    "        if any(df.iloc[k]>=limite) or any(df[row] >= limite):\n",
    "            g.node(str(k),row, shape='oval', fontsize='10', width='0') \n",
    "\n",
    "    for j, col in enumerate(df.columns):\n",
    "        for i, row in enumerate(df[col]):\n",
    "            if row  >=limite:\n",
    "                g.edge(str(i), str(j), label=str(np.round(row,3)),\\\n",
    "                       color=\"#000000{:02x}\".format(int(255)))\n",
    "                \n",
    "                       #* row//max(df.max()))\n",
    "    return g    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lim_index(cdf, lim):\n",
    "    summation = 0\n",
    "    index = 0\n",
    "    for i in np.arange(0, cdf.size):\n",
    "        if cdf[i] > lim:\n",
    "            index = i\n",
    "            break\n",
    "    return index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrogate(a):\n",
    "    a_diff = np.diff(a)\n",
    "    begin = np.where(a_diff > 0)[0]\n",
    "    end = np.where(a_diff < 0)[0]\n",
    "    \n",
    "    if begin.size > end.size:\n",
    "        end = np.append(end, a.size)\n",
    "    elif begin.size < end.size:\n",
    "        begin = np.insert(begin, 0, 0) \n",
    "    elif begin.size == 0 and end.size == 0:\n",
    "        return a.copy()\n",
    "    elif np.all(begin > end):\n",
    "        begin = np.insert(begin, 0, 0)\n",
    "        end = np.append(end, a.size)\n",
    "    \n",
    "    n_seq = np.max([begin.size, end.size])\n",
    "    a_surr = np.zeros(a.shape)\n",
    "    p_seq = np.random.randint(0, a.size - max(end - begin), size=n_seq)\n",
    "    for i in np.random.permutation(n_seq):\n",
    "        len_seq = end[i] - begin[i]\n",
    "        a_surr[p_seq[i]:p_seq[i] + len_seq] = a[begin[i]:end[i]]\n",
    "    return a_surr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method using stats model kde to return transfer etnropy value limit. That is, the 'x' value corresponding to P95\n",
    "def significance_test(k,l,h,sup_lim, n, a,b):\n",
    "    '''\n",
    "        significance_test(a,b,k,l,h,sup_lim, n)\n",
    "    '''\n",
    "    transferEntropies = []    \n",
    "    \n",
    "    np.random.seed(int(time.time()))\n",
    "    for i in np.arange(0,n):\n",
    "        surrogate_a = surrogate(a.copy())\n",
    "        transferEntropies.append(te(k,l,h,surrogate_a[:],b, 'serie_a', 'serie_b'))\n",
    "        \n",
    "    kde = sm.nonparametric.KDEUnivariate(transferEntropies)\n",
    "    kde.fit()\n",
    "    \n",
    "    \n",
    "    lvl_sig = kde.icdf[get_lim_index(kde.cdf, sup_lim)]\n",
    "    return lvl_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##for paper test\n",
    "def joint_probability_new(k,l,h, a, b, lbl_a, lbl_b):\n",
    "    '''\n",
    "        k B time horizon\n",
    "        l A time horizon\n",
    "        h instant in the future of serie B\n",
    "        \n",
    "        a, b array type'''\n",
    "    \n",
    "    numStates=2**(k+l+1)\n",
    "    combinations = list(map(list, itertools.product([0, 1], repeat=k+l+1)))\n",
    "    prob_cnjt = np.zeros(numStates)\n",
    "    \n",
    "    #Alarm Series A (cause), B (effect), same len\n",
    "    #teste   \n",
    "\n",
    "    matrix_nova = np.matrix([b[1:],b[:-1],a[:-1]]).T\n",
    "    df = pd.DataFrame(matrix_nova, columns = ['b_ftr', lbl_b, lbl_a])\n",
    "    gpd = df.groupby(['b_ftr', lbl_b, lbl_a], as_index=False).size().reset_index(name='Count')\n",
    "    total = sum(gpd['Count'])\n",
    "    \n",
    "    for i in np.arange(0,gpd.shape[0]):\n",
    "        comb = [e for e in gpd.iloc[i][0:3].values.tolist()]\n",
    "        idx = combinations.index(comb)\n",
    "        prob_cnjt[idx] = gpd.iloc[i]['Count']/total\n",
    "\n",
    "    return prob_cnjt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_probability(k,l, h, a, b):\n",
    "    '''\n",
    "        k B time horizon\n",
    "        l A time horizon\n",
    "        h instant in the future of serie B\n",
    "        \n",
    "        a, b array type'''\n",
    "\n",
    "    #Alarm Series A (cause), B (effect), same len\n",
    "    #tested\n",
    "    sizeSeries = a.size\n",
    "    transEntropy = 0\n",
    "    numStates = 2**(k + l  + 1)\n",
    "    combinations = list(map(list, itertools.product([0, 1], repeat=k+l+1)))\n",
    "    counting = np.zeros(numStates)\n",
    "    prob_cnjt = np.zeros(numStates)\n",
    "    a_prob_ind = []\n",
    "    b_prob_ind = []\n",
    "    #joitn probability p(i_sub_t+1), i_sub_t**k, j_sub_t**l)\n",
    "    inicio = np.max([k,l]) - 1\n",
    "    for i in np.arange(inicio, sizeSeries - h):\n",
    "        for hk in np.arange(0,k):\n",
    "                b_prob_ind.append(b[i - hk])\n",
    "        for hl in np.arange(0,l):\n",
    "                a_prob_ind.append(a[i - hl])\n",
    "\n",
    "        ab = [b[i + h]] + b_prob_ind + a_prob_ind \n",
    "        index_comb = combinations.index(ab)\n",
    "        counting[index_comb] = counting[index_comb] + 1\n",
    "\n",
    "        a_prob_ind = []\n",
    "        b_prob_ind = []\n",
    "\n",
    "    total = sum(counting)\n",
    "  \n",
    "    prob_cnjt = counting/total\n",
    "     \n",
    "    return prob_cnjt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joint probability evaluation p(i_t+h, i_t**k)\n",
    "#tested\n",
    "def joint_prob_ih_ik(k,l, joint_prob_ih_ik_jl):\n",
    "    states_ith_ik = list(map(list, itertools.product([0, 1], repeat=k + 1)))\n",
    "    combinations = list(map(list, itertools.product([0, 1], repeat=k+l+1))) \n",
    "    p_jnt_ith_ik = np.zeros(2**(k+1))\n",
    "    \n",
    "    for i, state in enumerate(states_ith_ik):\n",
    "        for j, comb in enumerate(combinations):\n",
    "            if comb[0:k+1] == state:\n",
    "                p_jnt_ith_ik[i] = p_jnt_ith_ik[i] + joint_prob_ih_ik_jl[j]\n",
    "    return p_jnt_ith_ik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_prob(k,l,joint_prob):\n",
    "    states = list(map(list, itertools.product([0, 1], repeat=k+l)))\n",
    "    combinations = list(map(list, itertools.product([0, 1], repeat=k+l+1)))\n",
    "\n",
    "    size = int(joint_prob.size/2)\n",
    "    conditional = np.zeros(2**(k+l+1))\n",
    "\n",
    "    for i,state in enumerate(states):\n",
    "        index_zero = combinations.index([0] + state)\n",
    "        prob_zero = joint_prob[index_zero]\n",
    "\n",
    "        index_one = combinations.index([1] + state)\n",
    "        prob_one = joint_prob[index_one]\n",
    "\n",
    "        if(prob_zero + prob_one != 0):\n",
    "            conditional[i] = prob_zero/(prob_zero+ prob_one)\n",
    "            conditional[i + 2**(k+l)] = prob_one/(prob_zero+ prob_one)\n",
    "    return conditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Division of the conditionals in log2 \n",
    "#tested\n",
    "def conditional_div(k,l,conditional_num, conditional_den):\n",
    "    combinations = list(map(list, itertools.product([0, 1], repeat=k+l+1)))\n",
    "    conditional_division = np.zeros(conditional_num.size)\n",
    "    states_den = list(map(list, itertools.product([0, 1], repeat=1+k)))\n",
    "    for j, comb in enumerate(combinations):\n",
    "        if(conditional_den[states_den.index(comb[0:k+1])] != 0):\n",
    "            conditional_division[j] = conditional_num[j]/conditional_den[states_den.index(comb[0:k+1])]            \n",
    "    return conditional_division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transfer entropy final evaluation\n",
    "def te(k,l,h,a,b, lbl_a, lbl_b):\n",
    "    '''\n",
    "        transentropy a->b\n",
    "        te(k,l,h,a,b)\n",
    "        k - dimension of b\n",
    "        l - dimension of aDataFrame count duplicate rows and ...\n",
    "        h -> instant in the future of a\n",
    "    '''\n",
    "    #joint_p_ih_ik_jl = joint_probability_new(k,l,h,a,b, lbl_a, lbl_b)\n",
    "    joint_p_ih_ik_jl = joint_probability(k,l,h,a,b)\n",
    "    \n",
    "    joint_p_ih_ik = joint_prob_ih_ik(k,l, joint_p_ih_ik_jl)\n",
    "    conditional_num = conditional_prob(k,l,joint_p_ih_ik_jl)\n",
    "    conditional_den = conditional_prob(k,0, joint_p_ih_ik)    \n",
    "    div = conditional_div(k,l,conditional_num, conditional_den)\n",
    "    \n",
    "    #log2 from the division of the conditionals -> #p(i_sub_t+h|i_sub_t**k, j_sub_t**l) /p(i_sub_t+h|i_t**k)\n",
    "    \n",
    "    log2_div_cond = np.log2(div[div!=0])\n",
    "    te = np.sum(joint_p_ih_ik_jl[div!=0]*log2_div_cond)\n",
    "    return te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transferEnteropy_case(dist_df, pcnt_sig, k, l, h):\n",
    "    start = time.clock()\n",
    "    transEntropy = np.zeros([dist_df.columns.size,dist_df.columns.size])\n",
    "    sigValues =  np.zeros([dist_df.columns.size,dist_df.columns.size])\n",
    "    for i in np.arange(0, dist_df.columns.size):\n",
    "        for j in np.arange(0, dist_df.columns.size):\n",
    "            print('trans ', dist_df.columns[i], dist_df.columns[j])\n",
    "            if(j != i + dist_df.columns.size/2 and j!=i and j != i - dist_df.columns.size/2):\n",
    "                transEntropy[i][j] = te(k,l,h,dist_df[dist_df.columns[i]],\n",
    "                                         dist_df[dist_df.columns[j]],'serie_a', 'serie_b')\n",
    "\n",
    "#                 sigValues[i][j] = significance_test(1,1,1,pcnt_sig,100,dist_df[dist_df.columns[i]],\n",
    "#                                                      dist_df[dist_df.columns[j]])\n",
    "            clear_output()\n",
    "            \n",
    "            \n",
    "#     for i, j in zip(np.arange(0, transEntropy.shape[0]), np.arange(0, sigValues.shape[0])):\n",
    "#         for k, l in zip(np.arange(0, transEntropy.shape[1]), np.arange(0, sigValues.shape[1])):\n",
    "#             if transEntropy[i][k] != 0 and transEntropy[i][k]<= sigValues[j][l]:\n",
    "#                 transEntropy[i][k] = 0\n",
    "    end = time.clock()   \n",
    "    \n",
    "    print(end - start)\n",
    "    return transEntropy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Resampling application - Method 1\n",
    "\n",
    "# df = dist_roll  #df which will be resampled\n",
    "# num_samp =  10 #number of samples that will be grouped\n",
    "# rate_samp = (num_samp*36)/3600 #rate in hours\n",
    "# total_samp = df.shape[0]*0.01/rate_samp\n",
    "\n",
    "# dist6_resamp = pd.DataFrame([], columns= df.columns)\n",
    "# for col in df.columns:\n",
    "#     dist6_resamp[col] = np.abs(np.round(sig.resample(df[col],int(np.round(total_samp)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resampling application - Method 2 \n",
    "\n",
    "df = dist_roll #df which will be resampled\n",
    "\n",
    "num_gpd_samples = 10 #num of samples to be grouped \n",
    "\n",
    "dist6_resamp = df.groupby(lambda i: i // num_gpd_samples).agg(lambda g: 0 if np.sum(g) < num_gpd_samples/2 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "window =3 #window of moving mean\n",
    "\n",
    "dist6_novo = pd.read_csv(\"dist6_3horas_sig/alm_seq.csv\")\n",
    "dist6_novo.drop('tout', axis=1, inplace=True)\n",
    "\n",
    "dist = dist6_novo[['xmeas%02d_low' % x for x in [1,2,3,8,9,21]] + ['xmeas%02d_high' % x for x in [1,2,3,8,9,21]]]\n",
    "roll  = dist.rolling(window).mean() \n",
    "roll.dropna(inplace=True)\n",
    "roll = roll.round(decimals=0,).copy()\n",
    "roll.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# df = dist_roll_3 #df which will be resampled\n",
    "\n",
    "df = roll\n",
    "num_gpd_samples = 1 #num of samples to be grouped \n",
    "\n",
    "dist_resamp= df.groupby(lambda i: i // num_gpd_samples).agg(lambda g: 0 if np.sum(g) < num_gpd_samples/2 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.97828000000027\n"
     ]
    }
   ],
   "source": [
    "te_matrix = transferEnteropy_case(dist_resamp, 0.99, 1, 1, 1 )\n",
    "\n",
    "df_te =  pd.DataFrame(te_matrix, columns = dist_resamp.columns, index= dist_resamp.columns)\n",
    "#plot_mat_corrs(figsize=(10,10), annot=True, matrix = df_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110.02469500000007\n"
     ]
    }
   ],
   "source": [
    "te_matrix_3_3 = transferEnteropy_case(dist_resamp, 0.99, 3, 3, 1)\n",
    "\n",
    "df_te_3 =  pd.DataFrame(te_matrix_3_3, columns = dist_resamp.columns, index= dist_resamp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "609.556728\n"
     ]
    }
   ],
   "source": [
    "te_matrix_6_6 = transferEnteropy_case(dist_resamp, 0.99, 6, 6, 1)\n",
    "\n",
    "df_te_6 =  pd.DataFrame(te_matrix_6_6, columns = dist_resamp.columns, index= dist_resamp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.99080500000127\n"
     ]
    }
   ],
   "source": [
    "te_matrix_1_3 = transferEnteropy_case(dist_resamp, 0.99, 1, 3, 1)\n",
    "\n",
    "df_te_1_3 =  pd.DataFrame(te_matrix_1_3, columns = dist_resamp.columns, index= dist_resamp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118.6265829999993\n"
     ]
    }
   ],
   "source": [
    "te_matrix_1_6 = transferEnteropy_case(dist_resamp, 0.99, 1, 6, 1)\n",
    "\n",
    "df_te_1_6 =  pd.DataFrame(te_matrix_1_6, columns = dist_resamp.columns, index= dist_resamp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.94110499999988\n"
     ]
    }
   ],
   "source": [
    "te_matrix_3_1 = transferEnteropy_case(dist_resamp, 0.99, 3, 1, 1)\n",
    "\n",
    "df_te_3_1 =  pd.DataFrame(te_matrix_3_1, columns = dist_resamp.columns, index= dist_resamp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125.11953600000015\n"
     ]
    }
   ],
   "source": [
    "te_matrix_6_1 = transferEnteropy_case(dist_resamp, 0.99, 6, 1, 1)\n",
    "\n",
    "df_te_6_1 =  pd.DataFrame(te_matrix_6_1, columns = dist_resamp.columns, index= dist_resamp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.650360999999975\n"
     ]
    }
   ],
   "source": [
    "te_matrix_h3 = transferEnteropy_case(dist_resamp, 0.99, 1, 1, 3)\n",
    "\n",
    "df_te_h3 =  pd.DataFrame(te_matrix_h3, columns = dist_resamp.columns, index= dist_resamp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.96563199999946\n"
     ]
    }
   ],
   "source": [
    "te_matrix_h6 = transferEnteropy_case(dist_resamp, 0.99, 1, 1, 6)\n",
    "\n",
    "df_te_h6 =  pd.DataFrame(te_matrix_h6, columns = dist_resamp.columns, index= dist_resamp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.53302799999983\n"
     ]
    }
   ],
   "source": [
    "te_matrix_k3_h3 = transferEnteropy_case(dist_resamp, 0.99, 3, 1, 3)\n",
    "\n",
    "df_te_k3_h3 =  pd.DataFrame(te_matrix_k3_h3, columns = dist_resamp.columns, index= dist_resamp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128.76338899999973\n"
     ]
    }
   ],
   "source": [
    "te_matrix_k6_h3 = transferEnteropy_case(dist_resamp, 0.99, 6, 1, 3)\n",
    "\n",
    "df_te_k6_h3 =  pd.DataFrame(te_matrix_k6_h3, columns = dist_resamp.columns, index= dist_resamp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.38560700000016\n"
     ]
    }
   ],
   "source": [
    "te_matrix_l3_h3 = transferEnteropy_case(dist_resamp, 0.99, 1, 3, 3)\n",
    "\n",
    "df_te_l3_h3 =  pd.DataFrame(te_matrix_l3_h3, columns = dist_resamp.columns, index= dist_resamp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121.68449900000087\n"
     ]
    }
   ],
   "source": [
    "te_matrix_l6_h3 = transferEnteropy_case(dist_resamp, 0.99, 1, 6, 3)\n",
    "\n",
    "df_te_l6_h3 =  pd.DataFrame(te_matrix_l6_h3, columns = dist_resamp.columns, index= dist_resamp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.73682399999961\n"
     ]
    }
   ],
   "source": [
    "te_matrix_k3_h6 = transferEnteropy_case(dist_resamp, 0.99, 3, 1, 6)\n",
    "\n",
    "df_te_k3_h6 =  pd.DataFrame(te_matrix_k3_h6, columns = dist_resamp.columns, index= dist_resamp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130.7821729999996\n"
     ]
    }
   ],
   "source": [
    "te_matrix_k6_h6 = transferEnteropy_case(dist_resamp, 0.99, 6, 1, 6)\n",
    "\n",
    "df_te_k6_h6 =  pd.DataFrame(te_matrix_k6_h6, columns = dist_resamp.columns, index= dist_resamp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.09240900000077\n"
     ]
    }
   ],
   "source": [
    "te_matrix_l3_h6 = transferEnteropy_case(dist_resamp, 0.99, 1, 3, 6)\n",
    "\n",
    "df_te_l3_h6 =  pd.DataFrame(te_matrix_l3_h6, columns = dist_resamp.columns, index= dist_resamp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131.93019400000048\n"
     ]
    }
   ],
   "source": [
    "te_matrix_l6_h6 = transferEnteropy_case(dist_resamp, 0.99, 1, 6, 6)\n",
    "\n",
    "df_te_l6_h6 =  pd.DataFrame(te_matrix_l6_h6, columns = dist_resamp.columns, index= dist_resamp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"208pt\" height=\"305pt\"\n",
       " viewBox=\"0.00 0.00 207.65 305.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 301)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-301 203.6541,-301 203.6541,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"61.327\" cy=\"-279\" rx=\"43.6222\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"61.327\" y=\"-276.5\" font-family=\"Times,serif\" font-size=\"10.00\" fill=\"#000000\">xmeas01_low</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>10</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"45.327\" cy=\"-105\" rx=\"45.1548\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"45.327\" y=\"-102.5\" font-family=\"Times,serif\" font-size=\"10.00\" fill=\"#000000\">xmeas09_high</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;10 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>0&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M59.4075,-260.8479C57.9554,-246.9373 55.9425,-227.2631 54.327,-210 51.9029,-184.0942 49.3947,-154.5681 47.6439,-133.4383\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"51.1112,-132.8965 46.8018,-123.2177 44.1349,-133.4714 51.1112,-132.8965\"/>\n",
       "<text text-anchor=\"middle\" x=\"69.827\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0.071</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>11</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"139.327\" cy=\"-192\" rx=\"45.1548\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"139.327\" y=\"-189.5\" font-family=\"Times,serif\" font-size=\"10.00\" fill=\"#000000\">xmeas21_high</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;11 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>0&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M76.7373,-261.8116C88.2752,-248.9425 104.2252,-231.152 117.1361,-216.7515\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"119.8793,-218.9348 123.9488,-209.1527 114.6673,-214.262 119.8793,-218.9348\"/>\n",
       "<text text-anchor=\"middle\" x=\"119.827\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0.091</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>5</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"99.327\" cy=\"-18\" rx=\"43.6222\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"99.327\" y=\"-15.5\" font-family=\"Times,serif\" font-size=\"10.00\" fill=\"#000000\">xmeas21_low</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>6</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"154.327\" cy=\"-105\" rx=\"45.1548\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"154.327\" y=\"-102.5\" font-family=\"Times,serif\" font-size=\"10.00\" fill=\"#000000\">xmeas01_high</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;5 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>6&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M143.197,-87.3943C135.3693,-75.0124 124.7596,-58.2297 115.9348,-44.2704\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"118.7426,-42.1619 110.4406,-35.5796 112.8258,-45.9024 118.7426,-42.1619\"/>\n",
       "<text text-anchor=\"middle\" x=\"141.827\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0.06</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;5 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>10&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M56.2548,-87.3943C63.9401,-75.0124 74.3569,-58.2297 83.0213,-44.2704\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"86.1157,-45.9218 88.4156,-35.5796 80.1682,-42.2302 86.1157,-45.9218\"/>\n",
       "<text text-anchor=\"middle\" x=\"90.827\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0.063</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;6 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>11&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M142.4351,-173.9735C144.4664,-162.1918 147.1614,-146.5607 149.4722,-133.1581\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"152.973,-133.4527 151.223,-123.0034 146.0748,-132.2633 152.973,-133.4527\"/>\n",
       "<text text-anchor=\"middle\" x=\"162.827\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0.064</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;10 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>11&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M121.2029,-175.2255C106.8984,-161.9862 86.7675,-143.3545 70.8355,-128.6089\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"73.112,-125.9469 63.3956,-121.723 68.3572,-131.0842 73.112,-125.9469\"/>\n",
       "<text text-anchor=\"middle\" x=\"112.827\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0.065</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fd34d738d68>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.mean(te_matrix_l6_h6) + 3*np.std(te_matrix_l6_h6)\n",
    "graph(df_te_l6_h6, t,'dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuracao 8 - somente periodos de disturbio (Media movel e resample)\n",
    "def select_dist_period(df, normal_w, abnormal_w):\n",
    "    df_disturb =  pd.DataFrame([], columns = df.columns)\n",
    "    series_dist = pd.Series([])\n",
    "    n = 0\n",
    "    for col in df.columns:\n",
    "        while n < df.shape[0]:\n",
    "            series_dist = series_dist.append(pd.Series(df[col][n:n+abnormal_w]))\n",
    "            n= n + normal_w + abnormal_w\n",
    "        df_disturb[col] = series_dist\n",
    "    return df_disturb\n",
    "dist6_c8 = select_dist_period(dist6_sel_vars, 800,200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joint probablity for functions test\n",
    "#joint_p_ih_ik_jl = np.array([0.97322404,0.00546448,0.00491803,0,0,0.00546448, 0.00546448, 0.00546448])\n",
    "\n",
    "#aproximate results for this test\n",
    "\n",
    "#p(ith, ik)\n",
    "#jnt_p_ih_ik = [0.97868852,0.00491803,0.00546448,0.0109286] \n",
    "\n",
    "#p(i_t+h|i**k, j**l)\n",
    "#cond_p_ih_ik_jl =  [1,0.5,0.4736841094123,0,0,0.5, 0.52631589085076,1]\n",
    "\n",
    "#p(i_th|i_k)\n",
    "#cond_p_ih_ik = [0.994711793480152,0.31035179088550,0.0552469991962,0.68964820911449]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"te_matrix_samp.csv\", te_matrix, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
